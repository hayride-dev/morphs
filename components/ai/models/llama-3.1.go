//go:build llama_3_1

package main

import (
	"fmt"
	"regexp"
	"strings"

	"github.com/hayride-dev/bindings/go/exports/ai/models"
	"github.com/hayride-dev/bindings/go/gen/types/hayride/ai/types"
	"go.bytecodealliance.org/cm"
)

var (
	// Define a regular expression to match the function name and the parameter list that may appear anywhere in the input string
	// Example: [func_name1(params_name1=params_value1, params_name2=params_value2)]
	parseFunc = regexp.MustCompile(`\[([a-zA-Z_][a-zA-Z0-9_@:.\-\/ ]*)\((.*)\)\]`)

	// The function definition for a custom defined function for llama 3.1
	// Example: <function=example:id/package example_function_name>{"example_name": "example_value"}</function>
	customFunc = regexp.MustCompile(`<function=(?P<pkg>[^\s>]+)\s+(?P<name>[^\s>]+)>\s*(?P<input>\{.*?\})?\s*<.*function>`)

	// Split the parameters by comma
	parseFuncParams = regexp.MustCompile(`[a-zA-Z_][a-zA-Z0-9_]*=('[^']*'|"[^"]*"|[^,]+)`)
)

const (
	// llama3.1 special tokens

	// specifies the start of the prompt
	beginOfText = "<|begin_of_text|>"

	// model will cease to generate more tokens. This token is generated only by the base models.
	endOfText = "<|end_of_text|>"

	// this token is used for padding text sequences to the same length in a batch
	finetuneRightPadID = "<|finetune_right_pad_id|>"

	// these token is used to enclose the role for a particular message.
	// the possible roles are: [system, user, assistant, ipython]
	startHeaderId = "<|start_header_id|>"
	endHeaderId   = "<|end_header_id|>"

	// end of message. A message represents a possible stopping point for execution where the
	// can inform the executor that a tool call needs to be made. This is used for multi-step interactions
	// between the model and any avilable tools. This token is emitted by the model when the environment: ipython
	// instruction is used in the system prompt, or if the model calls for a built-in tool.
	endOfMessage = "<|eom_id|>"

	// end of turn.  Represents when the model has determined that it has finished interacting with
	// the user message that initiated its response. This is used in two scenarios:
	//
	// at the end of a direct interaction between the model and the user
	// at the end of multiple interactions between the model and any available tools
	//
	// this token signals to the executor that the model has finished generating a response.
	endOfTurn = "<|eot_id|>"

	// special tag used in the modelâ€™s response to signify a tool call.
	pythonTag = "<|python_tag|>"

	//There are 4 different roles that are supported by Llama text models:
	//
	// system: Sets the context in which to interact with the AI model. It typically includes rules, guidelines, or necessary information that help the model respond effectively.
	// user: Represents the human interacting with the model. It includes the inputs, commands, and questions to the model.
	// ipython: A new role introduced in Llama 3.1. Semantically, this role means "tool". This role is used to mark messages with the output of a tool call when sent back to the model from the executor.
	// assistant: Represents the response generated by the AI model based on the context provided in the system, ipython and user prompts.
	system    = "system"
	user      = "user"
	tool      = "ipython"
	assistant = "assistant"

	// environment token
	env = "Environment: ipython"
)

var _ models.Formatter = &llama_3_1{}

func init() {
	m := &llama_3_1{}
	models.Export(m)
}

type llama_3_1 struct{}

func (m *llama_3_1) Decode(data []byte) (types.Message, error) {
	msg := string(data)
	if strings.Contains(msg, pythonTag) {
		// remove python tags
		content := strings.TrimPrefix(msg, pythonTag)
		content = strings.TrimSuffix(content, endOfMessage)

		matches := parseFunc.FindStringSubmatch(strings.TrimSpace(content))
		if len(matches) != 3 {
			return types.Message{}, fmt.Errorf("failed to parse assistant message, invalid function formation")
		}

		pkg := matches[0]
		name := matches[1]
		argsString := matches[2]

		paramsList := parseFuncParams.FindAllString(argsString, -1)
		var args []string
		for _, param := range paramsList {
			// Split each parameter into name and value
			paramParts := strings.SplitN(param, "=", 2)
			if len(paramParts) != 2 {
				return types.Message{}, fmt.Errorf("parameter format is invalid: %s", param)
			}
			// Trim spaces and quotes from value
			paramValue := strings.TrimSpace(paramParts[1])
			paramValue = strings.Trim(paramValue, "'")
			paramValue = strings.Trim(paramValue, "\"")
			args = append(args, paramValue)
		}
		return types.Message{
			Role: types.RoleAssistant,
			Content: cm.ToList([]types.Content{
				types.ContentToolInput(types.ToolInput{
					ID:          pkg,
					Name:        name,
					Input:       strings.Join(args, ","),
					ContentType: "tool-input",
				}),
			}),
		}, nil
	} else if strings.Contains(msg, "<function") {
		// Custom function definition
		matches := customFunc.FindStringSubmatch(msg)
		if len(matches) != 4 {
			return types.Message{}, fmt.Errorf("failed to parse assistant message, invalid function formation")
		}

		if matches != nil {
			result := make(map[string]string)
			for i, name := range customFunc.SubexpNames() {
				if i > 0 && name != "" {
					result[name] = matches[i]
				}
			}

			return types.Message{
				Role: types.RoleAssistant,
				Content: cm.ToList([]types.Content{
					types.ContentToolInput(types.ToolInput{
						ID:          result["pkg"],
						Name:        result["name"],
						Input:       result["input"],
						ContentType: "tool-input",
					}),
				}),
			}, nil
		} else {
			return types.Message{}, fmt.Errorf("failed to parse assistant message, invalid function formation")
		}
	}

	return types.Message{
		Role: types.RoleAssistant,
		Content: cm.ToList([]types.Content{
			types.ContentText(types.TextContent{
				Text:        msg,
				ContentType: "text",
			}),
		}),
	}, nil
}

func (m *llama_3_1) Encode(messages ...types.Message) ([]byte, error) {
	builder := &strings.Builder{}
	last := len(messages) - 1
	for i, msg := range messages {
		//builder.WriteString("<|begin_of_text|>")
		switch msg.Role {
		case types.RoleSystem:
			// set the system message
			builder.WriteString(fmt.Sprintf("%s%s%s\n", startHeaderId, system, endHeaderId))
			// add environment token to enable tools by default
			// TODO :: add ability to disable tool support in system prompt
			builder.WriteString(fmt.Sprintf("%s\n", env))

			// message body, collect tool schema definitions
			tools := []*types.ToolSchema{}
			for _, content := range msg.Content.Slice() {
				switch content.String() {
				case "text":
					c := content.Text()
					builder.WriteString(fmt.Sprintf("%s\n", c.Text))
				case "tool-schema":
					c := content.ToolSchema()
					tools = append(tools, c)
				}
			}

			if len(tools) > 0 {
				toolString := customToolEncode(tools)
				if toolString != "" {
					builder.WriteString(fmt.Sprintf("%s\n", toolString))
				}
			}

			// end system message turn
			builder.WriteString(endOfTurn)
		case types.RoleUser:
			// header
			builder.WriteString(fmt.Sprintf("%s%s%s\n", startHeaderId, user, endHeaderId))
			// message body ( user prompt )
			for _, content := range msg.Content.Slice() {
				if content.String() == "text" {
					c := content.Text()
					builder.WriteString(fmt.Sprintf("%s\n", c.Text))
				}
			}
			// end turn
			builder.WriteString(endOfTurn)
		case types.RoleAssistant:
			// header
			builder.WriteString(fmt.Sprintf("%s%s%s\n", startHeaderId, assistant, endHeaderId))
			// message body ( assistant response )
			for _, content := range msg.Content.Slice() {
				switch content.String() {
				case "text":
					c := content.Text()
					builder.WriteString(fmt.Sprintf("%s\n", c.Text))
					builder.WriteString(endOfTurn)
				case "tool-input":
					c := content.ToolInput()
					// TODO : support other function call ai
					builder.WriteString(fmt.Sprintf("<function=%s %s>%s<\\function>\n", c.ID, c.Name, c.Input))
					// end turn
					builder.WriteString(endOfMessage)
				}
			}
		case types.RoleTool:
			// header
			builder.WriteString(fmt.Sprintf("%s%s%s\n", startHeaderId, tool, endHeaderId))
			// message body ( tool output )
			for _, content := range msg.Content.Slice() {
				if content.String() == "tool-output" {
					c := content.ToolOutput()
					builder.WriteString(fmt.Sprintf("%s\n", c.Output))
				}
			}
			// end turn
			builder.WriteString(endOfTurn)
		default:
			return nil, fmt.Errorf("unknown supported message role: %v", msg.Role)
		}
		if i == last {
			if msg.Role != types.RoleAssistant {
				builder.WriteString(fmt.Sprintf("%s%s%s\n", startHeaderId, assistant, endHeaderId))
			}
		}
	}
	return []byte(builder.String()), nil
}

func customToolEncode(tools []*types.ToolSchema) string {
	if len(tools) == 0 {
		return ""
	}

	result := `
# Tool Instructions
- Function calls MUST follow the specified format
- Required parameters MUST be specified
- Only call one function at a time
- Put the entire function call reply on one line

You have access to the following functions:
{
	`
	for _, tool := range tools {
		result += fmt.Sprintf(`"id": "%s",\n`, tool.ID)
		result += fmt.Sprintf(`"name": "%s",\n`, tool.Name)
		result += fmt.Sprintf(`"description": "%s",\n`, tool.Description)
		if tool.ParamsSchema != "" {
			result += fmt.Sprintf(`"params": %s,\n`, tool.ParamsSchema)
		}
	}
	// remove last comma
	result = strings.TrimSuffix(result, ",\n")

	result += `
}
	
If a you choose to call a function ONLY reply in the following format:
<{start_tag}={function_id} {function_name}>{parameters}{end_tag}
where

start_tag => <function
parameters => a JSON dict with the function argument name as key and function argument value as value.
end_tag => </function>

Here is an example:
<function=example:id/package example_function_name>{"example_name": "example_value"}</function>
`
	return result
}

func main() {}
