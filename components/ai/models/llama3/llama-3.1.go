package llama3

import (
	"encoding/json"
	"fmt"
	"regexp"
	"strings"

	"github.com/hayride-dev/bindings/go/hayride/ai"
	"github.com/hayride-dev/bindings/go/hayride/ai/models"
	"github.com/hayride-dev/bindings/go/hayride/mcp"
	"go.bytecodealliance.org/cm"
)

var (
	// Define a regular expression to match the function name and the parameter list that may appear anywhere in the input string
	// Example: [func_name1(params_name1=params_value1, params_name2=params_value2)]
	parseFunc = regexp.MustCompile(`\[([a-zA-Z_][a-zA-Z0-9_@:.\-\/ ]*)\((.*)\)\]`)

	// The function definition for a custom defined function for llama 3.1
	// Example: <function=example_function_name>{"example_name": "example_value"}</function>
	customFunc = regexp.MustCompile(`<function=(?P<name>[^\s>]+)>\s*(?P<input>\{.*?\})?\s*<.*function>`)

	// Split the parameters by comma
	parseFuncParams = regexp.MustCompile(`[a-zA-Z_][a-zA-Z0-9_]*=('[^']*'|"[^"]*"|[^,]+)`)
)

const (
	// llama3.1 special tokens

	// specifies the start of the prompt
	beginOfText = "<|begin_of_text|>"

	// model will cease to generate more tokens. This token is generated only by the base models.
	endOfText = "<|end_of_text|>"

	// this token is used for padding text sequences to the same length in a batch
	finetuneRightPadID = "<|finetune_right_pad_id|>"

	// these token is used to enclose the role for a particular message.
	// the possible roles are: [system, user, assistant, ipython]
	startHeaderId = "<|start_header_id|>"
	endHeaderId   = "<|end_header_id|>"

	// end of message. A message represents a possible stopping point for execution where the
	// can inform the executor that a tool call needs to be made. This is used for multi-step interactions
	// between the model and any avilable tools. This token is emitted by the model when the environment: ipython
	// instruction is used in the system prompt, or if the model calls for a built-in tool.
	endOfMessage = "<|eom_id|>"

	// end of turn.  Represents when the model has determined that it has finished interacting with
	// the user message that initiated its response. This is used in two scenarios:
	//
	// at the end of a direct interaction between the model and the user
	// at the end of multiple interactions between the model and any available tools
	//
	// this token signals to the executor that the model has finished generating a response.
	endOfTurn = "<|eot_id|>"

	// special tag used in the modelâ€™s response to signify a tool call.
	pythonTag = "<|python_tag|>"

	//There are 4 different roles that are supported by Llama text models:
	//
	// system: Sets the context in which to interact with the AI model. It typically includes rules, guidelines, or necessary information that help the model respond effectively.
	// user: Represents the human interacting with the model. It includes the inputs, commands, and questions to the model.
	// ipython: A new role introduced in Llama 3.1. Semantically, this role means "tool". This role is used to mark messages with the output of a tool call when sent back to the model from the executor.
	// assistant: Represents the response generated by the AI model based on the context provided in the system, ipython and user prompts.
	system    = "system"
	user      = "user"
	tool      = "ipython"
	assistant = "assistant"

	// environment token
	env = "Environment: ipython"
)

var _ models.Format = (*llama3)(nil)

func Constructor_3_1() (models.Format, error) {
	return &llama3{}, nil
}

type llama3 struct{}

// Helper function to check if text ends with a complete sentence
func (m *llama3) endsWithCompleteSentence(text string) bool {
	text = strings.TrimSpace(text)
	if len(text) == 0 {
		return false
	}

	// Check for sentence-ending punctuation
	lastChar := text[len(text)-1]
	return lastChar == '.' || lastChar == '!' || lastChar == '?' || lastChar == ':' || lastChar == ';'
}

// Helper function to check if text appears to be mid-word
func (m *llama3) isMidWord(text string) bool {
	text = strings.TrimSpace(text)
	if len(text) == 0 {
		return false
	}

	// If text ends with a letter or digit (not punctuation/whitespace), it might be mid-word
	lastChar := text[len(text)-1]
	return (lastChar >= 'a' && lastChar <= 'z') ||
		(lastChar >= 'A' && lastChar <= 'Z') ||
		(lastChar >= '0' && lastChar <= '9')
}

func (m *llama3) Decode(data []byte) (*ai.Message, error) {
	text := string(data)

	// If we don't have any content, return partial decode error
	if len(strings.TrimSpace(text)) == 0 {
		return nil, &models.PartialDecodeError{}
	}

	// Check if we have a complete message structure
	// Look for header patterns to determine message structure
	if !strings.Contains(text, startHeaderId) {
		// Raw text without headers - could be streaming assistant response
		if strings.TrimSpace(text) != "" {
			trimmedText := strings.TrimSpace(text)

			// Check if this is a function call even without headers
			functionMatches := customFunc.FindAllStringSubmatch(trimmedText, -1)
			if len(functionMatches) > 0 {
				// Parse as function call
				return m.parseFunctionCallContent(trimmedText)
			}

			// For plain text, try to build complete sentences
			// If text doesn't end with complete sentence and appears to be mid-word/sentence,
			// return partial decode error to get more data
			if !m.endsWithCompleteSentence(trimmedText) &&
				(m.isMidWord(trimmedText) || len(trimmedText) < 10) {
				return nil, &models.PartialDecodeError{}
			}

			// Return as partial assistant message for streaming
			content := []ai.MessageContent{
				ai.NewMessageContent(ai.Text(trimmedText)),
			}

			return &ai.Message{
				Role:    ai.RoleAssistant,
				Content: cm.ToList(content),
			}, nil
		}

		return nil, &models.PartialDecodeError{}
	}

	// Parse structured message with headers
	return m.parseStructuredMessage(text)
}

func (m *llama3) parseFunctionCallContent(content string) (*ai.Message, error) {
	var messageContents []ai.MessageContent

	functionMatches := customFunc.FindAllStringSubmatch(content, -1)
	for _, match := range functionMatches {
		if len(match) < 3 {
			continue
		}

		functionName := match[1]
		jsonInput := match[2]

		// Parse JSON input into arguments
		var args map[string]interface{}
		if jsonInput != "" {
			if err := json.Unmarshal([]byte(jsonInput), &args); err != nil {
				return nil, fmt.Errorf("invalid function call JSON: %v", err)
			}
		}

		// Convert to tool call format using CallToolParams
		var arguments [][2]string
		for key, value := range args {
			valueStr := fmt.Sprintf("%v", value)
			arguments = append(arguments, [2]string{key, valueStr})
		}

		toolCall := mcp.CallToolParams{
			Name:      functionName,
			Arguments: cm.ToList(arguments),
		}

		messageContents = append(messageContents, ai.NewMessageContent(toolCall))
	}

	if len(messageContents) == 0 {
		return nil, &models.PartialDecodeError{}
	}

	return &ai.Message{
		Role:    ai.RoleAssistant,
		Content: cm.ToList(messageContents),
	}, nil
}

func (m *llama3) parseStructuredMessage(text string) (*ai.Message, error) {
	// Find the last complete message in the text
	headerPattern := regexp.MustCompile(startHeaderId + `([^<]+)` + endHeaderId)
	headerMatches := headerPattern.FindAllStringSubmatch(text, -1)

	if len(headerMatches) == 0 {
		return nil, &models.PartialDecodeError{}
	}

	// Get the last header match to find the current message being constructed
	lastHeader := headerMatches[len(headerMatches)-1]
	if len(lastHeader) < 2 {
		return nil, &models.PartialDecodeError{}
	}

	role := strings.TrimSpace(lastHeader[1])

	// Find the content after the last header
	lastHeaderStart := strings.LastIndex(text, lastHeader[0])
	if lastHeaderStart == -1 {
		return nil, &models.PartialDecodeError{}
	}

	// Extract content after the header
	contentStart := lastHeaderStart + len(lastHeader[0])
	if contentStart >= len(text) {
		// Header exists but no content yet
		return nil, &models.PartialDecodeError{}
	}

	content := text[contentStart:]

	// Check for end tokens to determine if message is complete
	hasEndToken := strings.Contains(content, endOfTurn) ||
		strings.Contains(content, endOfMessage) ||
		strings.Contains(content, endOfText)

	// Parse based on role
	switch role {
	case assistant:
		return m.parseAssistantMessage(content, hasEndToken)
	case system:
		return m.parseSystemMessage(content, hasEndToken)
	case user:
		return m.parseUserMessage(content, hasEndToken)
	case tool:
		return m.parseToolMessage(content, hasEndToken)
	default:
		return nil, &models.PartialDecodeError{}
	}
}

func (m *llama3) parseAssistantMessage(content string, hasEndToken bool) (*ai.Message, error) {
	// Clean content of end tokens for processing
	cleanContent := content
	cleanContent = strings.Replace(cleanContent, endOfTurn, "", -1)
	cleanContent = strings.Replace(cleanContent, endOfMessage, "", -1)
	cleanContent = strings.Replace(cleanContent, endOfText, "", -1)
	cleanContent = strings.TrimSpace(cleanContent)

	var messageContents []ai.MessageContent

	// Check for function calls using the custom function pattern
	functionMatches := customFunc.FindAllStringSubmatch(cleanContent, -1)

	if len(functionMatches) > 0 {
		// This is a function call message
		for _, match := range functionMatches {
			if len(match) < 3 {
				continue
			}

			functionName := match[1]
			jsonInput := match[2]

			// Parse JSON input into arguments
			var args map[string]interface{}
			if jsonInput != "" {
				if err := json.Unmarshal([]byte(jsonInput), &args); err != nil {
					// If we can't parse JSON and don't have end token, this might be partial
					if !hasEndToken {
						return nil, &models.PartialDecodeError{}
					}
					// If we have end token but invalid JSON, treat as error
					return nil, fmt.Errorf("invalid function call JSON: %v", err)
				}
			}

			// Convert to tool call format using CallToolParams
			var arguments [][2]string
			for key, value := range args {
				valueStr := fmt.Sprintf("%v", value)
				arguments = append(arguments, [2]string{key, valueStr})
			}

			toolCall := mcp.CallToolParams{
				Name:      functionName,
				Arguments: cm.ToList(arguments),
			}

			messageContents = append(messageContents, ai.NewMessageContent(toolCall))
		}

		// For function calls, we need the end of message token to be complete
		if !strings.Contains(content, endOfMessage) && !hasEndToken {
			return nil, &models.PartialDecodeError{}
		}
	} else {
		// This is a text response
		if cleanContent == "" {
			// Empty content, might be starting to generate
			if !hasEndToken {
				return nil, &models.PartialDecodeError{}
			}
		} else {
			// For text responses without end tokens, try to build complete sentences
			if !hasEndToken && !m.endsWithCompleteSentence(cleanContent) &&
				(m.isMidWord(cleanContent) || len(cleanContent) < 10) {
				return nil, &models.PartialDecodeError{}
			}
		}

		// For text responses, we can stream partial content
		if cleanContent != "" {
			messageContents = append(messageContents, ai.NewMessageContent(ai.Text(cleanContent)))
		}
	}

	if len(messageContents) == 0 {
		return nil, &models.PartialDecodeError{}
	}

	return &ai.Message{
		Role:    ai.RoleAssistant,
		Content: cm.ToList(messageContents),
	}, nil
}

func (m *llama3) parseSystemMessage(content string, hasEndToken bool) (*ai.Message, error) {
	// Clean content of end tokens
	cleanContent := strings.Replace(content, endOfTurn, "", -1)
	cleanContent = strings.TrimSpace(cleanContent)

	if cleanContent == "" && !hasEndToken {
		return nil, &models.PartialDecodeError{}
	}

	messageContents := []ai.MessageContent{
		ai.NewMessageContent(ai.Text(cleanContent)),
	}

	return &ai.Message{
		Role:    ai.RoleSystem,
		Content: cm.ToList(messageContents),
	}, nil
}

func (m *llama3) parseUserMessage(content string, hasEndToken bool) (*ai.Message, error) {
	// Clean content of end tokens
	cleanContent := strings.Replace(content, endOfTurn, "", -1)
	cleanContent = strings.TrimSpace(cleanContent)

	if cleanContent == "" && !hasEndToken {
		return nil, &models.PartialDecodeError{}
	}

	messageContents := []ai.MessageContent{
		ai.NewMessageContent(ai.Text(cleanContent)),
	}

	return &ai.Message{
		Role:    ai.RoleUser,
		Content: cm.ToList(messageContents),
	}, nil
}

func (m *llama3) parseToolMessage(content string, hasEndToken bool) (*ai.Message, error) {
	// Clean content of end tokens
	cleanContent := strings.Replace(content, endOfTurn, "", -1)
	cleanContent = strings.TrimSpace(cleanContent)

	if cleanContent == "" && !hasEndToken {
		return nil, &models.PartialDecodeError{}
	}

	// Tool messages are typically just text content
	messageContents := []ai.MessageContent{
		ai.NewMessageContent(ai.Text(cleanContent)),
	}

	return &ai.Message{
		Role:    ai.RoleTool,
		Content: cm.ToList(messageContents),
	}, nil
}

func (m *llama3) Encode(messages ...ai.Message) ([]byte, error) {
	builder := &strings.Builder{}

	for i, msg := range messages {
		switch msg.Role {
		case ai.RoleSystem:
			// System message header
			builder.WriteString(fmt.Sprintf("%s%s%s\n", startHeaderId, system, endHeaderId))

			// Add environment token to enable tools by default
			builder.WriteString(fmt.Sprintf("%s\n\n", env))

			// Process system message content
			tools := []mcp.Tool{}
			for _, content := range msg.Content.Slice() {
				switch content.String() {
				case "text":
					c := content.Text()
					builder.WriteString(fmt.Sprintf("%s\n", *c))
				case "tools":
					c := content.Tools().Slice()
					tools = c
				}
			}

			// Add tool definitions if available
			if len(tools) > 0 {
				toolString := customToolEncode(tools)
				if toolString != "" {
					builder.WriteString(fmt.Sprintf("%s", toolString))
				}
			}

			// End system message turn
			builder.WriteString(endOfTurn)

		case ai.RoleUser:
			// User message header
			builder.WriteString(fmt.Sprintf("%s%s%s\n", startHeaderId, user, endHeaderId))

			// Process user message content
			for _, content := range msg.Content.Slice() {
				if content.String() == "text" {
					c := content.Text()
					builder.WriteString(fmt.Sprintf("%s", *c))
				}
			}

			// End user message turn
			builder.WriteString(fmt.Sprintf("%s", endOfTurn))

		case ai.RoleAssistant:
			// Assistant message header
			builder.WriteString(fmt.Sprintf("%s%s%s\n", startHeaderId, assistant, endHeaderId))

			// Process assistant message content
			hasContent := false
			for _, content := range msg.Content.Slice() {
				switch content.String() {
				case "text":
					c := content.Text()
					if *c != "" {
						builder.WriteString(fmt.Sprintf("%s", *c))
						hasContent = true
					}

				case "tool-input":
					c := content.ToolInput()
					// Convert arguments to JSON format
					args := make(map[string]string)
					for _, arg := range c.Arguments.Slice() {
						if len(arg) != 2 {
							return nil, fmt.Errorf("invalid tool input argument format: %v", arg)
						}
						args[arg[0]] = arg[1]
					}

					input := "{}"
					if len(args) > 0 {
						jsonArgs, err := json.Marshal(args)
						if err != nil {
							return nil, fmt.Errorf("failed to marshal tool input arguments: %v", err)
						}
						input = string(jsonArgs)
					}

					builder.WriteString(fmt.Sprintf("<function=%s>%s</function>", c.Name, input))
					hasContent = true
				}
			}

			// End assistant message appropriately
			// If this is a tool call, use end of message token
			// If this is regular text, use end of turn token
			if i < len(messages)-1 && messages[i+1].Role == ai.RoleTool {
				builder.WriteString(endOfMessage)
			} else if hasContent {
				builder.WriteString(endOfTurn)
			}

		case ai.RoleTool:
			// Tool (ipython) message header
			builder.WriteString(fmt.Sprintf("%s%s%s\n", startHeaderId, tool, endHeaderId))

			// Process tool message content
			for _, content := range msg.Content.Slice() {
				switch content.String() {
				case "text":
					c := content.Text()
					builder.WriteString(fmt.Sprintf("%s", *c))

				case "tool-output":
					output := content.ToolOutput()
					for _, c := range output.Content.Slice() {
						switch c.String() {
						case "text":
							builder.WriteString(fmt.Sprintf("%s", c.Text().Text))
						case "image":
							image := c.Image()
							builder.WriteString(fmt.Sprintf("Image Data: %v", image.Data))
						case "audio":
							audio := c.Audio()
							builder.WriteString(fmt.Sprintf("Audio Data: %v", audio.Data))
						case "resource-link":
							resource := c.ResourceLink()
							builder.WriteString(fmt.Sprintf("Resource Link: %s", resource.URI))
						case "resource-content":
							content := c.ResourceContent()
							switch content.ResourceContents.String() {
							case "text":
								builder.WriteString(fmt.Sprintf("Resource Content (Text): %s", content.ResourceContents.Text()))
							case "blob":
								builder.WriteString(fmt.Sprintf("Resource Content (Blob): %v", content.ResourceContents.Blob()))
							}
						}
					}
				}
			}

			// End tool message turn
			builder.WriteString(endOfTurn)

		default:
			return nil, fmt.Errorf("unsupported message role: %v", msg.Role)
		}
	}

	// If the last message is not from assistant, add assistant header to prompt for response
	if len(messages) > 0 && messages[len(messages)-1].Role != ai.RoleAssistant {
		builder.WriteString(fmt.Sprintf("%s%s%s\n", startHeaderId, assistant, endHeaderId))
	}

	return []byte(builder.String()), nil
}

func customToolEncode(tools []mcp.Tool) string {
	if len(tools) == 0 {
		return ""
	}

	result := `
	# Tool Instructions
	- Calling a tool is not necessary, use relevant functions only if needed
	- If you call a function, put the entire function call reply on one line
	- Only add parameters when the params are specified in the tool schema
	- When you get a response from a tool, use that information to answer the user query

	You have access to the following functions:
	{
	`
	for _, tool := range tools {
		result += fmt.Sprintf(`"name": "%s",\n`, tool.Name)
		result += fmt.Sprintf(`"description": "%s",\n`, tool.Description)
		// Add input schema properties
		if len(tool.InputSchema.Properties.Slice()) > 0 {
			result += `"parameters": {`
			for i, prop := range tool.InputSchema.Properties.Slice() {
				result += fmt.Sprintf(`"%s":`, prop[0])
				result += fmt.Sprintf(`"%s"`, prop[1])
				if i < len(tool.InputSchema.Properties.Slice())-1 {
					result += ", "
				}
			}
			result += "},\n"
		} else {
			result += `"parameters": {},\n`
		}
	}

	// remove last comma
	result = strings.TrimSuffix(result, ",\n")

	result += `
	}
		
	If a you choose to call a function ONLY reply in the following format:
	<{start_tag}={function_name}>{parameters}{end_tag}
	where

	start_tag => <function
	parameters => a JSON dict with the function argument name as key and function argument value as value.
	end_tag => </function>

	Here is an example,
	<function=example_function_name>{"example_name": "example_value"}</function>

	Reminder:
	- Function calls MUST follow the specified format
	- Required parameters MUST be specified
	- Only call one function at a time
	- Put the entire function call reply on one line
	- Always add your sources when using search results to answer the user query
	`
	return result
}
